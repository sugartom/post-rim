docker run --gpus all --rm --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p8000:8000 -p8001:8001 -p8002:8002 --name trt_serving -v /home/yitao/Documents/fun-project/tensorflow-related/tensorrt-inference-server/docs/examples/model_repository:/models nvcr.io/nvidia/tensorrtserver:19.10-py3 trtserver --model-repository=/models
docker run --gpus device=1 --rm --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p8000:8000 -p8001:8001 -p8002:8002 --name trt_serving -v /home/yitao/Documents/fun-project/tensorflow-related/tensorrt-inference-server/docs/examples/model_repository:/models nvcr.io/nvidia/tensorrtserver:19.10-py3 trtserver --model-repository=/models

docker run -it --rm --net=host nvcr.io/nvidia/tensorrtserver:19.12-py3-clientsdk

image_client -m resnet50_netdef -s INCEPTION /workspace/images/mug.jpg
python /workspace/install/python/image_client.py -m resnet50_netdef -s INCEPTION /workspace/images/mug.jpg

perf_client -m resnet50_netdef --percentile=95 --concurrency-range 1:8
perf_client -m inception_graphdef --percentile=95 --concurrency-range 1:8

# To modify model setting
cd /home/yitao/Documents/fun-project/tensorflow-related/tensorrt-inference-server/docs/examples/model_repository
subl config.pbtxt

# Yolov3
docker run --gpus device=0 --rm --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p8000:8000 -p8001:8001 -p8002:8002 --name trt_serving -v /home/yitao/Documents/fun-project/tensorflow-related/post-rim/tensorrt-inference-server/model_repository:/models nvcr.io/nvidia/tensorrtserver:19.10-py3 trtserver --model-repository=/models